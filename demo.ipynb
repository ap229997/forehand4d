{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo script for the forecasting model\n",
    "\n",
    "### Follow the setup instructions in README.md to install dependencies and download the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from easydict import EasyDict\n",
    "\n",
    "# Set this path to your downloads directory which contains all the data & models\n",
    "os.environ[\"DOWNLOADS_DIR\"] = '../downloads_forehand4d' # \"<path to downloads directory>\"\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from loguru import logger\n",
    "\n",
    "import src.factory as factory\n",
    "from common.torch_utils import reset_all_seeds\n",
    "import common.data_utils as data_utils\n",
    "from common.args_utils import set_default_params, set_extra_params\n",
    "from src.parsers.generic_parser import add_generic_args\n",
    "import src.parsers.configs.mdm as config\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and set all the required args\n",
    "sys.argv = ['']\n",
    "parser = argparse.ArgumentParser()\n",
    "parser = add_generic_args(parser)\n",
    "args, unknown = parser.parse_known_args()\n",
    "args = EasyDict(vars(parser.parse_args()))\n",
    "\n",
    "default_args = config.DEFAULT_ARGS_EGO\n",
    "args = set_default_params(args, default_args) # only preserves keys from args\n",
    "args = set_extra_params(args, default_args) # also preserves new keys from default_args which are not present in args\n",
    "\n",
    "# default values\n",
    "args.exp_key = 'logs/mdm_demo'\n",
    "args.experiment = None\n",
    "args.img_norm_mean = [0.485, 0.456, 0.406]\n",
    "args.img_norm_std = [0.229, 0.224, 0.225]\n",
    "args.num_workers = 0\n",
    "args.batch_size = 1 # fix this for demo\n",
    "args.focal_length = 1000.0\n",
    "args.debug = False\n",
    "\n",
    "args.use_gt_k = True\n",
    "args.aug_data = False\n",
    "args.rot_factor = 0.0\n",
    "args.scale_factor = 1.0\n",
    "args.flip_prob = 0.0\n",
    "args.noise_factor = 1.0\n",
    "augm_dict = data_utils.augm_params(\n",
    "            args.aug_data,\n",
    "            args.flip_prob,\n",
    "            args.noise_factor,\n",
    "            args.rot_factor,\n",
    "            args.scale_factor,\n",
    "        )\n",
    "use_gt_k = args.use_gt_k\n",
    "\n",
    "from torchvision.transforms import Normalize\n",
    "normalize_img = Normalize(mean=args.img_norm_mean, std=args.img_norm_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup cuda memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "args.method = 'mdm_light'\n",
    "args.load_ckpt = f'{os.environ[\"DOWNLOADS_DIR\"]}/model/forehand4d/model_v2.ckpt'\n",
    "\n",
    "args.seed = random.randint(0, 100000)\n",
    "reset_all_seeds(args.seed)\n",
    "torch.set_num_threads(args.num_threads)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "wrapper = factory.fetch_model(args).to(device)\n",
    "\n",
    "ckpt = torch.load(args.load_ckpt)\n",
    "wrapper.load_state_dict(ckpt[\"state_dict\"])\n",
    "logger.info(f\"Loaded weights from {args.load_ckpt}\")\n",
    "wrapper = wrapper.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image with hand-object scenario\n",
    "img_dir = './assets/examples'\n",
    "img_files = glob(os.path.join(img_dir, '*.jpg'))\n",
    "\n",
    "imgname = random.choice(img_files)\n",
    "\n",
    "# load image\n",
    "cv_img, img_status = data_utils.read_img(imgname, (2800, 2000, 3))\n",
    "\n",
    "# process image into the required format\n",
    "img_w, img_h = cv_img.shape[1], cv_img.shape[0]\n",
    "center = [img_w // 2, img_h // 2]\n",
    "scale = max(img_w, img_h) / 200.0\n",
    "rgb_img = data_utils.rgb_processing(\n",
    "            args.aug_data,\n",
    "            cv_img,\n",
    "            center,\n",
    "            scale,\n",
    "            augm_dict,\n",
    "            img_res=args.img_res,\n",
    "        )\n",
    "print ('Original image shape:', cv_img.shape)\n",
    "print ('Processed image shape:', rgb_img.shape)\n",
    "# plot cv_img and rgb_img side by side\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(cv_img.astype(np.uint8))\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('Original Image')\n",
    "ax[1].imshow((255*rgb_img).transpose(1, 2, 0).astype(np.uint8))\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('Processed Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the required inputs to the model\n",
    "torch_img = torch.from_numpy(rgb_img).float()\n",
    "norm_img = normalize_img(torch_img)\n",
    "inputs = {}\n",
    "targets = {}\n",
    "meta_info = {}\n",
    "inputs[\"img\"] = norm_img.unsqueeze(0) # history size is 1\n",
    "meta_info[\"imgname\"] = imgname\n",
    "\n",
    "# define ground truth camera intrinsics, this is for the 224x224 image, not the original image\n",
    "# this value is for ARCTIC\n",
    "intrx = np.array([[192.79396,   0.     , 112.32694],\n",
    "                  [0.     , 192.7464 , 103.70519],\n",
    "                  [0.     ,   0.     ,   1.     ]], dtype=np.float32)\n",
    "meta_info[\"intrinsics\"] = torch.FloatTensor(intrx).unsqueeze(0)\n",
    "\n",
    "# these are required so that code doesn't break\n",
    "is_valid, left_valid, right_valid = 1, 1, 1\n",
    "num_joints = 21\n",
    "targets[\"is_valid\"] = torch.tensor([float(is_valid)])\n",
    "targets[\"left_valid\"] = torch.tensor([float(left_valid) * float(is_valid)])\n",
    "targets[\"right_valid\"] = torch.tensor([float(right_valid) * float(is_valid)])\n",
    "targets[\"joints_valid_r\"] = (torch.ones((num_joints)) * targets[\"right_valid\"]).unsqueeze(0)\n",
    "targets[\"joints_valid_l\"] = (torch.ones((num_joints)) * targets[\"left_valid\"]).unsqueeze(0)\n",
    "\n",
    "vis_timesteps = 60 # number of timesteps to visualize\n",
    "meta_info['mask_timesteps'] = torch.zeros(args.max_motion_length).bool()\n",
    "meta_info['mask_timesteps'][:vis_timesteps] = True\n",
    "meta_info['lengths'] = torch.tensor([vis_timesteps], dtype=torch.int32)\n",
    "targets['future_valid_r'] = torch.ones(args.max_motion_length, num_joints)\n",
    "targets['future_valid_l'] = torch.ones(args.max_motion_length, num_joints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model does not predict betas, but need these for getting MANO mesh\n",
    "# mean values of beta, computed from val set of arctic, used for datasets without MANO fits\n",
    "# can also use default beta values in MANO, either is fine as long as it is consistent across training\n",
    "mean_beta_r = [0.82747316,  0.13775729, -0.39435294, 0.17889787, -0.73901576, 0.7788163, -0.5702684, 0.4947751, -0.24890041, 1.5943261]\n",
    "mean_beta_l = [-0.19330633, -0.08867972, -2.5790455, -0.10344583, -0.71684015, -0.28285977, 0.55171007, -0.8403888, -0.8490544, -1.3397144]\n",
    "targets['future_betas_r'] = torch.tensor(mean_beta_r).unsqueeze(0).repeat(args.max_motion_length, 1)\n",
    "targets['future_betas_l'] = torch.tensor(mean_beta_l).unsqueeze(0).repeat(args.max_motion_length, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to device and add batch dimension\n",
    "def to_device(data, device):\n",
    "    for k, v in data.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            data[k] = v.to(device)\n",
    "    return data\n",
    "\n",
    "def unsqueeze_batch(data):\n",
    "    for k, v in data.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            data[k] = v.unsqueeze(0)\n",
    "        elif isinstance(v, float):\n",
    "            data[k] = torch.tensor([v]).to(device)\n",
    "    return data\n",
    "\n",
    "inputs = to_device(inputs, device)\n",
    "targets = to_device(targets, device)\n",
    "meta_info = to_device(meta_info, device)\n",
    "\n",
    "# unsqueeze batch dimension\n",
    "inputs = unsqueeze_batch(inputs)\n",
    "targets = unsqueeze_batch(targets)\n",
    "meta_info = unsqueeze_batch(meta_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "bz = meta_info['mask_timesteps'].shape[0]\n",
    "num_samples = 5 # number of samples to generate from diffusion model\n",
    "wrapper.max_vis_examples = bz\n",
    "all_vis_dict = wrapper.inference(inputs, targets, meta_info, num_samples=num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the generated motion\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl' \n",
    "from src.callbacks.vis.visualize_arctic import visualize_motion_viz\n",
    "vis_fn = visualize_motion_viz\n",
    "all_vis_imgs = []\n",
    "for n, vis_dict in enumerate(tqdm(all_vis_dict)):\n",
    "    curr_im_list = vis_fn(\n",
    "        vis_dict,\n",
    "        wrapper.max_vis_examples,\n",
    "        wrapper.renderer,\n",
    "        postfix='',\n",
    "        no_tqdm=True,\n",
    "        only_hands=True,\n",
    "    )\n",
    "\n",
    "    for b in range(bz):\n",
    "        imgname = curr_im_list[b]['fig_name']\n",
    "        imgidx = imgname.split('/')[-1].split('.')[0]\n",
    "\n",
    "        img = curr_im_list[b]['im'] # (4*H, W, 3)\n",
    "        inp = curr_im_list[b]['inp_img']\n",
    "        save_name = os.path.join(img_dir, f'{n:02d}.png')\n",
    "        # separate into 4 images\n",
    "        img = np.split(img, 4, axis=0)\n",
    "        img = np.hstack(img).astype(np.uint8)\n",
    "        inp = (inp * 255).astype(np.uint8)\n",
    "\n",
    "        # concatenate inp and img\n",
    "        combined = np.hstack((inp, img)).clip(0, 255).astype(np.uint8)\n",
    "        all_vis_imgs.append(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lighter shades denote father away timesteps\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(num_samples, 1, figsize=(15, 15))\n",
    "for i in range(num_samples):\n",
    "    ax[i].imshow(all_vis_imgs[i])\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
